#!/usr/bin/python3

from Robot import RobotControlInput
from .AbstractNavAlgo import AbstractNavigationAlgorithm
import numpy as np
import copy
import Vector

def generic_value_iteration(mdp, reward, gamma=0.98, max_iter=1000, min_iter=0, threshold=0.01):

	def softmax_qvals(q_values):
		max_qval = max(q_values[action] for action in q_values)
		exp_qvals = {action: np.exp(qval - max_qval) for action, qval in q_values.items()}
		return max(exp_qvals.values())/sum(exp_qvals.values())
		

	# Basic placeholder reward function
	if reward is None:
		reward = lambda state, action: mdp.reward(state, action, None)

	old_values = {state: 0.0 for state in mdp.states()}
	old_values[mdp.goal_state()] = 1
	new_values = old_values

	qvals = dict()
	for state in mdp.states():
		qvals[state] = dict()
		for action in mdp.actions(state):
			qvals[state][action] = 0.0

	iteration = 0
	while (iteration < max_iter):
		old_values = new_values
		new_values = dict()
		for state in mdp.states():
			for action in mdp.actions(state):
				# Fear not: this massive line is just a Bellman-ish update
				qvals[state][action] = reward(state, action) + gamma*sum(mdp.transition_prob(state, action, next_state)*old_values[next_state] for next_state in mdp.successors(state))

			## Softmax to get value
			max_qval = max(qvals[state][action] for action in qvals[state])
			exp_qvals = {action: np.exp(qval - max_qval) for action, qval in qvals[state].items()}
			new_values[state] = sum(qvals[state][action]*exp_qvals[action]/sum(exp_qvals.values()) for action in exp_qvals)

			# Just take the max to get values
			#new_values[state] = max(qvals[state].values())

		if max(abs(old_values[s] - new_values[s]) for s in mdp.states()) < threshold and min_iter <= iteration:
			break

		iteration += 1

	policy = dict()
	for state in mdp.states():
		policy[state] = dict()
		max_qval = max(qvals[state].values())
		exp_qvals = {action: np.exp(qval-max_qval) for action, qval in qvals[state].items()}
		#exp_qvals = {action: qval for action, qval in qvals[state].items()}
		sum_exp_qvals = sum(exp_qvals.values())
		for action in mdp.actions(state):
			#print(policy[state], exp_qvals, qvals[state])
			policy[state][action] = exp_qvals[action]/sum_exp_qvals if sum_exp_qvals != 0 else 1.0/len(mdp.actions(state))

	return policy


## Value Iteration navigation algorithm
# This initially runs value iteration to get an optimal policy, then it
# executes the policy.
#
class ValueIterationNavigationAlgorithm(AbstractNavigationAlgorithm):

	## Initializes the navigation algorithm.
	# 
	# @param sensors (dict of sensors)
	# <br>	-- the sensors that this algorithm has access to
	#
	# @param target (Target obj)
	# <br>	-- the target for the navigation
	#
	# @param cmdargs (object)
	# <br>	-- A command-line arguments object generated by `argparse`.
	# 
	def __init__(self, sensors, target, cmdargs, real_algo_init=None):
		self._sensors = sensors;
		self._target  = target;
		self._cmdargs = cmdargs;

		self._mdp = sensors['mdp'];
		self._gps = sensors['gps'];
		self.debug_info = self._sensors['debug']

		self._values = {k: 0.0 for k in self._mdp.states()}
		self._values[self._mdp.goal_state()] = 1

		self._policy = self._do_value_iter()


	def _do_value_iter(self):
		def reward_func(state, action):
			return self._mdp.reward(state, action, None)
		return generic_value_iteration(self._mdp, reward_func, gamma=0.97, threshold=0.05, min_iter=300, max_iter=5000)


	## Select the next action for the robot
	#
	# This function uses the robot's radar and location information, as
	# well as internally stored information about previous locations,
	# to compute the next action the robot should take.
	#
	# @returns (`Robot.RobotControlInput` object)
	# <br>	-- A control input representing the next action the robot
	# 	should take.
	#
	def select_next_action(self):
		state = self._get_state();
		action = self._get_action(state);

		return RobotControlInput(action[1], action[0]);


	## Gets the state representation
	#
	def _get_state(self):
		loc = self._gps.location()
		return self._mdp.discretize(loc)


	## Gets the action based on the policy
	#
	# Since the policy is stochastic, note that this chooses a random
	# action based on the policy's probability distribution
	def _get_action(self, state):
		total = 0.0
		rand = np.random.random()
		for action in self._policy[state]:
			total += self._policy[state][action]
			if rand < total:
				return action
		return self._policy[state].keys()[0]

	## Adds a (state, action) pair to the current demonstration for the IRL
	# algorithm.
	#
	def add_demonstration_step(self, state, max_steps):
		# returns sequence of (state, action, next_state, reward)
		# until goal is reached, or max number of steps taken
		sequence = []
		steps = 0
		while state != self._mdp.goal_state() and steps < max_steps:
		    # return (s, a, s', r)
		    action = self._get_action(state)
		    next_state = self._mdp.get_successor_state(state, action)
		    reward = self._mdp.reward(state, action, next_state)
		    step = (state, action, next_state, reward)
		    sequence.append(step)
		    state = next_state
		    steps += 1
		return sequence


	def has_given_up(self):
		return False;



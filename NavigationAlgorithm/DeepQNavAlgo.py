#!/usr/bin/python3

import numpy  as np
import cntk
#import cntk.contrib
import cntk_deeprl
import gym.spaces as gs
import Vector
from .AbstractNavAlgo import AbstractNavigationAlgorithm
from RobotControlInput import RobotControlInput


## A navigation algorithm to be used with robots, based on deep q-learning.
#
# @see
# \ref AbstractNavAlgo.AbstractNavigationAlgorithm
# 	"AbstractNavigationAlgorithm"
#
class DeepQNavigationAlgorithm(AbstractNavigationAlgorithm):

	## Initializes the navigation algorithm.
	# 
	# @param sensors (dict of sensors)
	# <br>	-- the sensors that this algorithm has access to
	#
	# @param target (Target obj)
	# <br>	-- the target for the navigation
	#
	# @param cmdargs (object)
	# <br>	-- A command-line arguments object generated by `argparse`.
	# 
	def __init__(self, sensors, target, cmdargs):
		cntk.try_set_default_device(cntk.device.gpu(1));
		self._sensors = sensors;
		self._cmdargs = cmdargs;
		self._target = target;

		self._radar   = self._sensors['radar'];
		self._radar_data = None
		self._dynamic_radar_data = None

		self._gps     = self._sensors['gps'];

		self._normal_speed = float(cmdargs.robot_speed);

		self.debug_info = {};

		self._stepNum = 0;
		self._need_q_start = True;

		self._o_space = gs.Box(low=0, high=100, shape=(2,360));
		self._a_space = gs.Discrete(8);
		self._qlearner = cntk_deeprl.agent.qlearning.QLearning('', self._o_space, self._a_space);
		self._get_observation();
		self._last_badness = self._get_badness();
		self._ravg = [0] * 50
		self._epsilon = 0.0


	## Next action selector method.
	#
	# @see 
	# \ref AbstractNavAlgo.AbstractNavigationAlgorithm.select_next_action
	# 	"AbstractNavigationAlgorithm.select_next_action()"
	#
	def select_next_action(self):
		self._stepNum += 1;
		observation = self._get_observation();
		direction = 0;
		if self._need_q_start:
			self._need_q_start = False;
			return RobotControlInput(self._normal_speed, self._qlearner.start(observation)[0])

		cur_badness = self._get_badness();
		action = self._qlearner.step(-1 if np.amin(self._radar_data) < 11 else 0, observation);
		#print(action)
		direction = action[0] * 45
		if action[1]['action_behavior'] != 'RANDOM' or True:
			#self._ravg = self._ravg*0.85 + ((-1) if np.amin(self._radar_data) < 11 else 0)
			self._ravg.append((-1) if np.amin(self._radar_data) < 11 else 0)
			self._ravg.pop(0)
			print(sum(self._ravg)/len(self._ravg))
		self._epsilon = action[1]['epsilon']
		self._last_badness = self._get_badness();

#		if Vector.distance_between(self._gps.location(), self._target.position) < 20:
#			self._qlearner.end(1);
#			self._need_q_start = True;

		return RobotControlInput(self._normal_speed, direction);

	def _get_badness(self):
		return float(np.min(self._radar_data)/100.0)-1.0

	def _get_observation(self):
		onehot_goaldirection = np.zeros(360)
		onehot_goaldirection[int(Vector.degrees_between(self._gps.location(), self._target.position))] = Vector.distance_between(self._gps.location(), self.target.position)/200.0
		self._radar_data = self._radar.scan(self._gps.location());
		return np.vstack([self._radar_data/100.0, onehot_goaldirection]);

	def set_target(self, new_target):
		if not self._need_q_start:
			reward = 1 if Vector.distance_between(self._gps.location(), self._target.position) < 25.9 else 0
			self._ravg.append(reward)
			self._ravg.pop(0)
			if np.random.uniform(0, 1) < 0.2:
				self._qlearner.end(reward, self._get_observation());
				#self._ravg = self._ravg*0.85 + 1
				print('reward end', self._ravg, self._epsilon)
				self._need_q_start = True;
			else:
				self._qlearner.step(reward, self._get_observation());
				#self._ravg = self._ravg*0.85 + reward
				print(sum(self._ravg)/len(self._ravg), self._epsilon)
		self._target = new_target

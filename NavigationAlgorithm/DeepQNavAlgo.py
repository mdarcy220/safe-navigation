#!/usr/bin/python3

import numpy  as np
import cntk
import cntk.contrib
import gym.spaces as gs
import Vector
from .AbstractNavAlgo import AbstractNavigationAlgorithm
from RobotControlInput import RobotControlInput


## A navigation algorithm to be used with robots, based on deep q-learning.
#
# @see
# \ref AbstractNavAlgo.AbstractNavigationAlgorithm
# 	"AbstractNavigationAlgorithm"
#
class DeepQNavigationAlgorithm(AbstractNavigationAlgorithm):

	## Initializes the navigation algorithm.
	# 
	# @param sensors (dict of sensors)
	# <br>	-- the sensors that this algorithm has access to
	#
	# @param target (Target obj)
	# <br>	-- the target for the navigation
	#
	# @param cmdargs (object)
	# <br>	-- A command-line arguments object generated by `argparse`.
	# 
	def __init__(self, sensors, target, cmdargs):
		self._sensors = sensors;
		self._cmdargs = cmdargs;
		self._target = target;

		self._radar   = self._sensors['radar'];
		self._radar_data = None
		self._dynamic_radar_data = None

		self._gps     = self._sensors['gps'];

		self._normal_speed = float(cmdargs.robot_speed);

		self.debug_info = {};

		self._stepNum = 0;
		self._need_q_start = True;

		self._o_space = gs.Box(low=0, high=100, shape=(2,360));
		self._a_space = gs.Discrete(360);
		self._qlearner = cntk.contrib.deeprl.agent.qlearning.QLearning('', self._o_space, self._a_space);
		self._get_observation();
		self._last_badness = self._get_badness();
		self._ravg = 0.0


	## Next action selector method.
	#
	# @see 
	# \ref AbstractNavAlgo.AbstractNavigationAlgorithm.select_next_action
	# 	"AbstractNavigationAlgorithm.select_next_action()"
	#
	def select_next_action(self):
		self._stepNum += 1;
		observation = self._get_observation();
		direction = 0;
		if self._need_q_start:
			self._need_q_start = False;
			return RobotControlInput(self._normal_speed, self._qlearner.start(observation)[0])

		cur_badness = self._get_badness();
		action = self._qlearner.step(cur_badness - self._last_badness, observation);
		direction = action[0]
		if action[1]['action_behavior'] != 'RANDOM':
			self._ravg = self._ravg*0.85 + (cur_badness - self._last_badness)
			print(self._ravg)
		self._last_badness = self._get_badness();

#		if Vector.distance_between(self._gps.location(), self._target.position) < 20:
#			self._qlearner.end(1);
#			self._need_q_start = True;

		return RobotControlInput(self._normal_speed, direction);

	def _get_badness(self):
		return float(np.min(self._radar_data)/100.0)-1.0

	def _get_observation(self):
		onehot_goaldirection = np.zeros(360)
		onehot_goaldirection[int(Vector.degrees_between(self._gps.location(), self._target.position))] = 1
		self._radar_data = self._radar.scan(self._gps.location());
		return np.vstack([self._radar_data, onehot_goaldirection]);

	def set_target(self, new_target):
		self._target = new_target
		if not self._need_q_start:
			self._qlearner.end(1, self._get_observation());
			#self._ravg = self._ravg*0.85 + 1
			print('reward end', self._ravg)
			self._need_q_start = True;

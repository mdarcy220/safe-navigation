#!/usr/bin/python3

import numpy  as np
import cntk
#import cntk.contrib
import cntk_deeprl
import gym.spaces as gs
import Vector
from .AbstractNavAlgo import AbstractNavigationAlgorithm
from RobotControlInput import RobotControlInput
import random
import math
import matplotlib.pyplot as plt
import os

## A navigation algorithm to be used with robots, based on deep q-learning.
#
# @see
# \ref AbstractNavAlgo.AbstractNavigationAlgorithm
# 	"AbstractNavigationAlgorithm"
#
class DeepQIRLAlgorithm(AbstractNavigationAlgorithm):

	## Initializes the navigation algorithm.
	# 
	# @param sensors (dict of sensors)
	# <br>	-- the sensors that this algorithm has access to
	#
	# @param target (Target obj)
	# <br>	-- the target for the navigation
	#
	# @param cmdargs (object)
	# <br>	-- A command-line arguments object generated by `argparse`.
	# 
	def __init__(self, sensors, target, cmdargs):
		try:
		    cntk.try_set_default_device(cntk.device.gpu(1));
		except:
		    cntk.try_set_default_device(cntk.device.cpu())
		self._sensors = sensors;
		self._cmdargs = cmdargs;
		self._target = target;

		#self._radar   = self._sensors['radar'];
		#self._radar_data = None
		#self._dynamic_radar_data = None

		#self._gps     = self._sensors['gps'];

		self._normal_speed = float(cmdargs.robot_speed);

		self.debug_info = {};

		self._stepNum = 0;
		self._mdp = self._sensors['mdp'];
		self._features = self._get_features();
		# needs at least 7 features for qlearning to work
		self._o_space_shape= (1,self._features[random.sample(self._mdp.states(),1)[0]].size)

		self._o_space = gs.Box(low=0, high=100, shape=self._o_space_shape);
		self._a_space = gs.Discrete(4);
		#self.learner = cntk_deeprl.agent.policy_gradient.ActorCritic # actor critic trainer
		self.learner = cntk_deeprl.agent.qlearning.QLearning # qlearning trainer
		if self.learner == cntk_deeprl.agent.qlearning.QLearning:
			self._qlearner = self.learner('local_configs/deepq_1.ini', self._o_space, self._a_space);
		elif self.learner == cntk_deeprl.agent.policy_gradient.ActorCritic:
			self._qlearner = self.learner('local_configs/polify_gradient_1.ini', self._o_space, self._a_space);
		else:
			raise TypeError("Invalid type for _qlearner")

		self.maxIter = 100000
		self._policy = self.get_policy();


	## Next action selector method.
	#
	# @see 
	# \ref AbstractNavAlgo.AbstractNavigationAlgorithm.select_next_action
	# 	"AbstractNavigationAlgorithm.select_next_action()"
	#
	def select_next_action(self):
		self._stepNum += 1;
		return RobotControlInput(0, 0)

	def _get_features(self):
		mdp = self._mdp
		features = mdp._features
		return features

	def get_policy(self):
		mdp = self._mdp
		goal_state = mdp._goal_state
		max_dist = math.sqrt(mdp._width ** 2 + mdp._height ** 2)
		updates = 0
		counter = 0
		current_state = (mdp._goal_state[0]-1,mdp._goal_state[1]-1)
		start_position = current_state
		actions_set = mdp.actions(current_state)
		actions = list([action for action in actions_set])
		current_observation = np.vstack(self._features[current_state]).T
		current_action = self._qlearner.start(current_observation)

		for iteration in range(self.maxIter):
			#print(iteration)
			
			action = actions[current_action[0]]
			next_state = mdp.get_successor_state(current_state,action)

			reward_temp = mdp.reward(next_state, action, None)
			reward = max_dist - math.sqrt((next_state[0] - goal_state[0]) ** 2 + (next_state[1] - goal_state[1]) ** 2)
			reward = reward / max_dist
			reward += reward_temp

			#if mdp._walls [next_state[1],next_state[0]] == 1 :
			#	reward = -1
			
			observation = np.vstack(self._features[next_state]).T
			#print (observation[0,0:4], observation[0,6:10])
			if (any(i>97 for i in observation[0,0:4]) or any(i>97 for i in observation[0,6:10])):
				reward -= 0.2
			counter += 1
			#print(action, next_state, reward)
			dist_goal = (next_state[0] - mdp._goal_state[0], next_state[1] - mdp._goal_state[1])
			goal_diff = True if dist_goal[0]<2 or dist_goal[1]<2 else False
			if goal_diff or next_state == mdp._goal_state or counter > min(math.sqrt(updates) * 60,1000):
				counter =0
				#print (iteration, updates)
				self._qlearner.end(reward,observation)
				updates += 1 
				#current_state = random.choice(mdp.states())
				current_state = self.start_position(updates)
				start_position = current_state
				observation = np.vstack(self._features[current_state]).T
				#for _ in range(1000):
				#	self._qlearner._adjust_exploration_rate()
				#policy = self.calc_policy(iteration,actions,start_position)

				self._qlearner.start(observation)
			else:
				current_action = self._qlearner.step(reward,observation)
				#print (current_action[1])
				current_state = next_state
			if iteration%1000 == 0 and iteration>0:
				self.calc_policy(iteration,actions,start_position)
		return self.calc_policy(0,actions,start_position)
	
	def start_position(self, iteration):
		dispersion_factor = 0.3
		dispersion = iteration * dispersion_factor
		goal = self._mdp._goal_state
		x_range = (max(0,int(goal[0] - dispersion)), min (self._mdp._width-1,int( goal[0]+dispersion)))
		y_range = (max(0,int(goal[1] - dispersion)), min (self._mdp._height-1,int( goal[1]+dispersion)))
		while True:
			newPos = (random.randint(x_range[0],x_range[1]), random.randint(y_range[0],y_range[1]))
			if self._mdp._walls[newPos[1],newPos[0]] == 0:
				return newPos

	def calc_policy(self, iteration, actions, start_position):
		mdp = self._mdp
		policy = dict()
		rewards = np.zeros((1,mdp._height * mdp._width))
		for state in mdp.states():
			(x,y) = state
			rewards[0,y*mdp._width + x] = 1 if state == mdp._goal_state else 0
			rewards[0,y*mdp._width + x] += 0.5 if state == start_position else 0
			observation = np.vstack(self._features[state]).T
			if self.learner == cntk_deeprl.agent.policy_gradient.ActorCritic: # actor critic trainer
				qvals_actions = self._qlearner._evaluate_model(self._qlearner._policy_network, observation)
			elif self.learner == cntk_deeprl.agent.qlearning.QLearning: # qlearning trainer
				qvals_actions = self._qlearner._evaluate_q(self._qlearner._q,observation)
			sum_qvals = sum(qvals_actions)
			#print (qvals_actions)
			#break
			
			policy[state] = dict()
			for i,action in enumerate(actions):
				policy[state][action] = qvals_actions[i]/sum_qvals#math.exp(qvals_actions[i] - sum_qvals)

		self.plot_reward_policy(rewards, policy, iteration)
		return policy
		
	def plot_reward_policy(self, reward_map, policy, iteration=0, dpi=196.0):
		# Set up the figure
		plt.gcf().set_dpi(dpi)
		ax = plt.axes()

		# Note that we're messing with the input args here
		reward_map = reward_map.reshape(self._mdp._height, self._mdp._width)
		plt.imshow(reward_map, cmap='hot', interpolation='nearest')

		# The scale controls the size of the arrows
		scale = 0.8

		for state in self._mdp.states():
			# Init a dict of the values for each action
			action_values = {action:policy[state][action] for action in self._mdp.actions(state)}

			# avgarrow points in the average direction the robot
			# will travel based on the stochastic policy
			avgarrow_vec = np.sum(item[1]*Vector.unit_vec_from_degrees(item[0][0]) for item in action_values.items())
			avgarrow_mag = Vector.magnitudeOf(avgarrow_vec)
			avgarrow_vec = avgarrow_vec/avgarrow_mag
			ax.arrow(state[0], state[1], avgarrow_vec[0]*0.1, avgarrow_vec[1]*0.1, head_width = scale * max(min(1,avgarrow_mag),0.3), head_length = scale * max(min(1,avgarrow_mag),0.3), color='r')

			# maxarrow points in the single most likely direction
			max_action = max((item for item in action_values.items()), key=lambda item: item[1])
			maxarrow_vec = Vector.unit_vec_from_degrees(max_action[0][0])
			ax.arrow(state[0], state[1], 0.1*maxarrow_vec[0], 0.1*maxarrow_vec[1], head_width= scale * max(min(1,max_action[1]),0.3), head_length = scale *max(min(1,max_action[1]),0.3), color='g')

		# Output the figure to the image file
		plt.savefig('../output_data/r_p{:02d}.png'.format(iteration))
		plt.close()



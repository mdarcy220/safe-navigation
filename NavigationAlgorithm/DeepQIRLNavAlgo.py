#!/usr/bin/python3

import numpy  as np
import cntk
#import cntk.contrib
import cntk_deeprl
import gym.spaces as gs
import Vector
from .AbstractNavAlgo import AbstractNavigationAlgorithm
from RobotControlInput import RobotControlInput
import random

## A navigation algorithm to be used with robots, based on deep q-learning.
#
# @see
# \ref AbstractNavAlgo.AbstractNavigationAlgorithm
# 	"AbstractNavigationAlgorithm"
#
class DeepQIRLAlgorithm(AbstractNavigationAlgorithm):

	## Initializes the navigation algorithm.
	# 
	# @param sensors (dict of sensors)
	# <br>	-- the sensors that this algorithm has access to
	#
	# @param target (Target obj)
	# <br>	-- the target for the navigation
	#
	# @param cmdargs (object)
	# <br>	-- A command-line arguments object generated by `argparse`.
	# 
	def __init__(self, sensors, target, cmdargs):
		try:
		    cntk.try_set_default_device(cntk.device.gpu(1));
		except:
		    cntk.try_set_default_device(cntk.device.cpu())
		self._sensors = sensors;
		self._cmdargs = cmdargs;
		self._target = target;

		self._radar   = self._sensors['radar'];
		self._radar_data = None
		self._dynamic_radar_data = None

		self._gps     = self._sensors['gps'];

		self._normal_speed = float(cmdargs.robot_speed);

		self.debug_info = {};

		self._stepNum = 0;
		self._need_q_start = True;
		self._mdp = self._sensors['mdp'];
		self._features = self._get_features();
		# needs at least 7 features for qlearning to work
		self._o_space_shape= (1,self._features[random.sample(self._mdp.states(),1)[0]].size)

		self._o_space = gs.Box(low=0, high=100, shape=self._o_space_shape);
		self._a_space = gs.Discrete(4);
		self._qlearner = cntk_deeprl.agent.qlearning.QLearning('', self._o_space, self._a_space);
		#self._get_observation();
		#self._last_badness = self._get_badness();
		self._ravg = [0] * 50
		self._epsilon = 0.0
		self.maxIter = 10000
		self.get_policy();


	## Next action selector method.
	#
	# @see 
	# \ref AbstractNavAlgo.AbstractNavigationAlgorithm.select_next_action
	# 	"AbstractNavigationAlgorithm.select_next_action()"
	#
	def select_next_action(self):
		self._stepNum += 1;
		observation = self._get_observation();
		direction = 0;
		if self._need_q_start:
			self._need_q_start = False;
			return RobotControlInput(self._normal_speed, self._qlearner.start(observation)[0])

		print("test")
		cur_badness = self._get_badness();
		action = self._qlearner.step(-1 if np.amin(self._radar_data) < 11 else 0, observation);
		#print(action)
		direction = action[0] * 45
		if action[1]['action_behavior'] != 'RANDOM' or True:
			#self._ravg = self._ravg*0.85 + ((-1) if np.amin(self._radar_data) < 11 else 0)
			self._ravg.append((-1) if np.amin(self._radar_data) < 11 else 0)
			self._ravg.pop(0)
			print(sum(self._ravg)/len(self._ravg))
		self._epsilon = action[1]['epsilon']
		self._last_badness = self._get_badness();

#		if Vector.distance_between(self._gps.location(), self._target.position) < 20:
#			self._qlearner.end(1);
#			self._need_q_start = True;

		return RobotControlInput(self._normal_speed, direction);

	def _get_badness(self):
		return float(np.min(self._radar_data)/100.0)-1.0

	def _get_observation(self):
		onehot_goaldirection = np.zeros(360)
		onehot_goaldirection[int(Vector.degrees_between(self._gps.location(), self._target.position))] = Vector.distance_between(self._gps.location(), self._target.position)/200.0
		self._radar_data = self._radar.scan(self._gps.location());
		return np.vstack([self._radar_data/100.0, onehot_goaldirection]);

	def set_target(self, new_target):
		mdp = self._mdp
		if not self._need_q_start:
			reward = mdp.reward(state,action,next_state)
			self._ravg.append(reward)
			self._ravg.pop(0)
			if np.random.uniform(0, 1) < 0.2:
				self._qlearner.end(reward, self._get_observation());
				#self._ravg = self._ravg*0.85 + 1
				print('reward end', self._ravg, self._epsilon)
				self._need_q_start = True;
			else:
				self._qlearner.step(reward, self._get_observation());
				#self._ravg = self._ravg*0.85 + reward
				print(sum(self._ravg)/len(self._ravg), self._epsilon)
		self._target = new_target
	
	def _get_features(self):
		mdp = self._mdp
		features = mdp._features
		return features
	
	def _get_action(self, state, policy):
		total = 0.0
		rand = np.random.random()
		for action in policy[state]:
			total += self._policy[state][action]
			if rand < total:
				return action
		return list(policy[state].keys())[random.randint(0,3)]

	def get_policy(self):
		mdp = self._mdp
		updates = 0
		current_state = mdp._start_state
		actions_set = mdp.actions(current_state)
		actions = list([action for action in actions_set])
		current_observation = np.vstack(self._features[current_state]).T
		current_action = self._qlearner.start(current_observation)
		for iteration in range(self.maxIter):
			action = actions[current_action[0]]
			next_state = mdp.get_successor_state(current_state,action)

			reward = mdp.reward(next_state, action, None)
			observation = np.vstack(self._features[next_state]).T
			if next_state == mdp._goal_state:
				self._qlearner.end(reward,observation)
				self._qlearner._replay_and_update()
				updates += 1 
				current_state = random.choice(mdp.states())
				observation = np.vstack(self._features[current_state]).T
				for _ in range(1000):
					self._qlearner._adjust_exploration_rate()

				self._qlearner.start()
			else:
				current_action = self._qlearner.step(reward,observation)
				print (current_action[1])
				current_state = next_state
		qvals = dict()
		for state in mdp.states():
			observation = np.vstack(self._features[state]).T
			qvals_actions = self._qlearner._evaluate_q(self._qlearner._q,observation)
			print (qvals_actions)
			#break
			#qvals[state] = dict()
			#for i,action in enumerate(actions):
				#qvals[state][action] = 



			
			


